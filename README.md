# NetHub

#### Why am I reading this?
Because you are trying to **learn about neural networks**, but you've found out that it's a broad field cramped with terms and abbreviations.

#### About NetHub
NetHub is a  **list containing commonly reoccurring terms in the context of neural networks**; anything from methods, techniques and general terms that you often see but lack explanation. The aim of this list is to allow people studying neural networks to quickly look up terms, their meanings and related articles.

#### Tip
<kbd>Ctrl</kbd> + <kbd>F</kbd>

#### Sections
* [Abbreviations](#abbreviations)
* [General Terminology](#general-terminology)
* [Convolutional Neural Networks, CNN](#cnn)
* [Distributed Training](#distributed-training)
* [Other](#other)

---

### Abbreviations
* NN - Neural Network
* DNN - Deep Neural Network
* CNN - Convolutional Neural Network
* RNN - Recurrent Neural Networks
* R-CNN - Regions with Convolutional Neural Networks
* SPPnet - Spatial Pyramid Pooling Network
* LCN - Local Contrast Normalization
* SGD - Stochastic Gradient Descent
* ReLU - Rectified Linear Unit
* MSE - Mean Squared Error
* SS - Selective Search

---

### General Terminology
* [Forward Pass]()
* [Back Propagation]()
* [Output Function]()

#### Activation Function
* [tanh]()
* [Sigmoid function]()
* [ReLU]()
* [MaxOut]()

#### Loss Function
* [MSE]()
* [Cross Entropy]()

---

### CNN

#### Pooling
* [MaxPooling]()
* [AvgPooling]()
* [LpPooling]()

#### Data Normalization, LCN
* [Divisive Normalization, 2008](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0040027)
* [Subtractive Normalization, 2009](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf)
* [Whitening]()

#### Learning Rate Techniques
* [RMSProp, 2015](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
* [Adam, 2014](http://stanford.edu/~jduchi/projects/DuchiHaSi10_colt.pdf)
* [AdaGrad, 2011](http://stanford.edu/~jduchi/projects/DuchiHaSi10_colt.pdf)

#### Overfitting Avoidance Techniques
* [DropOut]()

#### Methods
* [Spatial Pyramid Pooling, SPPnet](http://arxiv.org/abs/1406.4729)
* [R-CNN](http://arxiv.org/abs/1311.2524), [Fast R-CNN](http://arxiv.org/abs/1504.08083), [Faster R-CNN](http://arxiv.org/abs/1506.01497)
* [OverFeat](TODO)
* [MultiBox](TODO)
* [EdgeBoxes, EB](TODO)
* [Selective Search, SS](http://dl.acm.org/citation.cfm?id=2509382)

---

### Distributed Training
* [Distributed Learning of Multilingual DNN Feature Extractors using GPUs, 2014](https://www.cs.cmu.edu/~ymiao/pub/dist_draft_final.pdf)
* [Large Scale Distributed Deep Networks, 2012](http://research.google.com/archive/large_deep_networks_nips2012.html)

---

### Other
* [UFLDL Tutorial](http://deeplearning.stanford.edu/tutorial/)
* [UFLDL Wiki](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)
* [Oxford Visual Geometry Group Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/#part1-4)
* [CNN Tutorial with Lasagne, Python, 2015](http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/)
